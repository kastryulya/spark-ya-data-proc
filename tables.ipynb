{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb707ae-878a-4954-ba6a-12aa52f3d7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:44:46.597517Z",
     "iopub.status.busy": "2026-02-11T04:44:46.596489Z",
     "iopub.status.idle": "2026-02-11T04:44:46.697954Z",
     "shell.execute_reply": "2026-02-11T04:44:46.696927Z",
     "shell.execute_reply.started": "2026-02-11T04:44:46.597454Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lalala\n"
     ]
    }
   ],
   "source": [
    "print('Lalala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a690e9d-617f-4dca-87ee-ca0920b6a0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:46:30.789205Z",
     "iopub.status.busy": "2026-02-11T04:46:30.787707Z",
     "iopub.status.idle": "2026-02-11T04:46:31.772679Z",
     "shell.execute_reply": "2026-02-11T04:46:31.771611Z",
     "shell.execute_reply.started": "2026-02-11T04:46:30.789150Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "io.delta.sql.DeltaSparkSessionExtension\n",
      "org.apache.spark.sql.delta.catalog.DeltaCatalog\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.sql.extensions\"))\n",
    "print(spark.conf.get(\"spark.sql.catalog.spark_catalog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaaff3a6-8dc0-4633-be0e-b52d4ef0ab5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:53:12.941057Z",
     "iopub.status.busy": "2026-02-10T12:53:12.939466Z",
     "iopub.status.idle": "2026-02-10T12:53:16.028398Z",
     "shell.execute_reply": "2026-02-10T12:53:16.027209Z",
     "shell.execute_reply.started": "2026-02-10T12:53:12.940991Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create database module_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9280e597-c696-44c6-a9d7-0b6ab2e24c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:56:38.091642Z",
     "iopub.status.busy": "2026-02-10T12:56:38.090499Z",
     "iopub.status.idle": "2026-02-10T12:57:05.775360Z",
     "shell.execute_reply": "2026-02-10T12:57:05.773493Z",
     "shell.execute_reply.started": "2026-02-10T12:56:38.091586Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/02/10 12:56:50 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/02/10 12:57:04 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `module_4`.`lesson43` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_DDLtable_query = \\\n",
    "'''\n",
    "create or replace table module_4.lesson43 (\n",
    "    id int,\n",
    "    run_params string,\n",
    "    status string,\n",
    "    status_message string\n",
    ")\n",
    "using delta\n",
    "'''\n",
    "\n",
    "spark.sql(create_DDLtable_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7f77a77-b352-4ebb-aa79-d00e8550f18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T12:59:10.623141Z",
     "iopub.status.busy": "2026-02-10T12:59:10.621552Z",
     "iopub.status.idle": "2026-02-10T12:59:10.945483Z",
     "shell.execute_reply": "2026-02-10T12:59:10.944348Z",
     "shell.execute_reply.started": "2026-02-10T12:59:10.623078Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                             |comment|\n",
      "+----------------------------+------------------------------------------------------+-------+\n",
      "|id                          |int                                                   |       |\n",
      "|run_params                  |string                                                |       |\n",
      "|status                      |string                                                |       |\n",
      "|status_message              |string                                                |       |\n",
      "|                            |                                                      |       |\n",
      "|# Partitioning              |                                                      |       |\n",
      "|Not partitioned             |                                                      |       |\n",
      "|                            |                                                      |       |\n",
      "|# Detailed Table Information|                                                      |       |\n",
      "|Name                        |module_4.lesson43                                     |       |\n",
      "|Location                    |s3a://yc-dataproc-data1/warehouse/module_4.db/lesson43|       |\n",
      "|Provider                    |delta                                                 |       |\n",
      "|Owner                       |jupyter                                               |       |\n",
      "|Table Properties            |[delta.minReaderVersion=1,delta.minWriterVersion=2]   |       |\n",
      "+----------------------------+------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe formatted module_4.lesson43').show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89268e69-849d-4b6a-8b9c-781b73926ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:01:29.613714Z",
     "iopub.status.busy": "2026-02-10T13:01:29.612604Z",
     "iopub.status.idle": "2026-02-10T13:01:30.838395Z",
     "shell.execute_reply": "2026-02-10T13:01:30.836627Z",
     "shell.execute_reply.started": "2026-02-10T13:01:29.613656Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+-----------------+-----------+------------------------------------------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name             |description|location                                              |createdAt              |lastModified       |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+-----------------+-----------+------------------------------------------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |e334da84-2ba2-401e-aa01-6a5e9610d614|module_4.lesson43|null       |s3a://yc-dataproc-data1/warehouse/module_4.db/lesson43|2026-02-10 12:56:39.615|2026-02-10 12:56:41|[]              |0       |0          |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+-----------------+-----------+------------------------------------------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe detail module_4.lesson43').show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ee866d-b39e-4292-aa8a-f3789157897d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:46:48.321839Z",
     "iopub.status.busy": "2026-02-11T04:46:48.320462Z",
     "iopub.status.idle": "2026-02-11T04:47:05.529941Z",
     "shell.execute_reply": "2026-02-11T04:47:05.528831Z",
     "shell.execute_reply.started": "2026-02-11T04:46:48.321788Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+-----------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp          |userId|userName|operation              |operationParameters                                                          |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                 |userMetadata|engineInfo                         |\n",
      "+-------+-------------------+------+--------+-----------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|3      |2026-02-10 13:08:44|null  |null    |UPDATE                 |{predicate -> (id#1738 = 2)}                                                 |null|null    |null     |2          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 1228, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 4268, scanTimeMs -> 2874, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1242, rewriteTimeMs -> 1390}|null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|2      |2026-02-10 13:06:37|null  |null    |WRITE                  |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |1          |Serializable  |true         |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 2456}                                                                                                                                                                      |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|1      |2026-02-10 13:06:17|null  |null    |WRITE                  |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |0          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 1228}                                                                                                                                                                      |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|0      |2026-02-10 12:56:41|null  |null    |CREATE OR REPLACE TABLE|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|null    |null     |null       |Serializable  |true         |{}                                                                                                                                                                                                                               |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "+-------+-------------------+------+--------+-----------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe history module_4.lesson43').show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a304c95a-08d3-40c4-adf8-bb751ce2c038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:06:34.243286Z",
     "iopub.status.busy": "2026-02-10T13:06:34.241503Z",
     "iopub.status.idle": "2026-02-10T13:06:44.530115Z",
     "shell.execute_reply": "2026-02-10T13:06:44.528847Z",
     "shell.execute_reply.started": "2026-02-10T13:06:34.243237Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('insert into module_4.lesson43 values (2, \\'lala1\\', \\'lala2\\', \\'lala3\\'), (3, \\'lala1\\', \\'lala2\\', \\'lala3\\')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64615546-0adb-40d0-9444-e78de3d60aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:08:55.092758Z",
     "iopub.status.busy": "2026-02-10T13:08:55.091235Z",
     "iopub.status.idle": "2026-02-10T13:08:58.466917Z",
     "shell.execute_reply": "2026-02-10T13:08:58.465775Z",
     "shell.execute_reply.started": "2026-02-10T13:08:55.092697Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+--------------+\n",
      "| id|run_params|status|status_message|\n",
      "+---+----------+------+--------------+\n",
      "|  2|   lalala2| lala2|         lala3|\n",
      "|  1|     lala1| lala2|         lala3|\n",
      "|  3|     lala1| lala2|         lala3|\n",
      "+---+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('module_4.lesson43').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb081ad1-1039-460c-aa5c-f8fc7024459c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:08:39.753301Z",
     "iopub.status.busy": "2026-02-10T13:08:39.751782Z",
     "iopub.status.idle": "2026-02-10T13:08:50.446457Z",
     "shell.execute_reply": "2026-02-10T13:08:50.445238Z",
     "shell.execute_reply.started": "2026-02-10T13:08:39.753241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>num_affected_rows</th></tr>\n",
       "<tr><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+\n",
       "|num_affected_rows|\n",
       "+-----------------+\n",
       "|                1|\n",
       "+-----------------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('update module_4.lesson43 set run_params = \\'lalala2\\' where id = 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bc8764-05d3-4671-9ece-44a168f766c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:51:25.137817Z",
     "iopub.status.busy": "2026-02-11T04:51:25.136477Z",
     "iopub.status.idle": "2026-02-11T04:51:25.235165Z",
     "shell.execute_reply": "2026-02-11T04:51:25.233412Z",
     "shell.execute_reply.started": "2026-02-11T04:51:25.137763Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "create or replace temp view lesson43_update as\n",
    "select * from values \n",
    "  (4, 'wrong params', 'Failed', 'Error: OutOfMemoryException'),\n",
    "  (1, 'upload test table', 'Success', 'Table was uploaded'),\n",
    "  (3, 'correct settings', 'Success', 'Empty message'),\n",
    "  (5, 'correct settings', 'Failed', 'Unavailable source cluster')\n",
    "as tab(id, run_params, status, status_message)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636777b4-fd70-47aa-af2a-f5c57cc2b4c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:51:56.134607Z",
     "iopub.status.busy": "2026-02-11T04:51:56.132989Z",
     "iopub.status.idle": "2026-02-11T04:51:58.053610Z",
     "shell.execute_reply": "2026-02-11T04:51:58.052480Z",
     "shell.execute_reply.started": "2026-02-11T04:51:56.134548Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+--------------+\n",
      "| id|run_params|status|status_message|\n",
      "+---+----------+------+--------------+\n",
      "|  2|   lalala2| lala2|         lala3|\n",
      "|  1|     lala1| lala2|         lala3|\n",
      "|  3|     lala1| lala2|         lala3|\n",
      "+---+----------+------+--------------+\n",
      "\n",
      "+---+-----------------+-------+--------------------+\n",
      "| id|       run_params| status|      status_message|\n",
      "+---+-----------------+-------+--------------------+\n",
      "|  4|     wrong params| Failed|Error: OutOfMemor...|\n",
      "|  1|upload test table|Success|  Table was uploaded|\n",
      "|  3| correct settings|Success|       Empty message|\n",
      "|  5| correct settings| Failed|Unavailable sourc...|\n",
      "+---+-----------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from module_4.lesson43').show()\n",
    "\n",
    "spark.sql('select * from lesson43_update').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6b1203-fefa-4be7-90d9-760247a6f73c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:52:07.060110Z",
     "iopub.status.busy": "2026-02-11T04:52:07.058497Z",
     "iopub.status.idle": "2026-02-11T04:52:19.672563Z",
     "shell.execute_reply": "2026-02-11T04:52:19.671197Z",
     "shell.execute_reply.started": "2026-02-11T04:52:07.060062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/02/11 04:52:10 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "26/02/11 04:52:11 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "26/02/11 04:52:11 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr>\n",
       "<tr><td>4</td><td>2</td><td>0</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+----------------+----------------+-----------------+\n",
       "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
       "+-----------------+----------------+----------------+-----------------+\n",
       "|                4|               2|               0|                2|\n",
       "+-----------------+----------------+----------------+-----------------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "merge into module_4.lesson43\n",
    "using lesson43_update\n",
    "on module_4.lesson43.id = lesson43_update.id\n",
    "when matched then update set *\n",
    "when not matched then insert *\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a55b6d7-be0c-401d-abf3-e8d94730d899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:52:41.031655Z",
     "iopub.status.busy": "2026-02-11T04:52:41.030108Z",
     "iopub.status.idle": "2026-02-11T04:52:42.822108Z",
     "shell.execute_reply": "2026-02-11T04:52:42.820902Z",
     "shell.execute_reply.started": "2026-02-11T04:52:41.031605Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-------+--------------------+\n",
      "| id|       run_params| status|      status_message|\n",
      "+---+-----------------+-------+--------------------+\n",
      "|  1|upload test table|Success|  Table was uploaded|\n",
      "|  3| correct settings|Success|       Empty message|\n",
      "|  4|     wrong params| Failed|Error: OutOfMemor...|\n",
      "|  5| correct settings| Failed|Unavailable sourc...|\n",
      "|  2|          lalala2|  lala2|               lala3|\n",
      "+---+-----------------+-------+--------------------+\n",
      "\n",
      "+---+-----------------+-------+--------------------+\n",
      "| id|       run_params| status|      status_message|\n",
      "+---+-----------------+-------+--------------------+\n",
      "|  4|     wrong params| Failed|Error: OutOfMemor...|\n",
      "|  1|upload test table|Success|  Table was uploaded|\n",
      "|  3| correct settings|Success|       Empty message|\n",
      "|  5| correct settings| Failed|Unavailable sourc...|\n",
      "+---+-----------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from module_4.lesson43').show()\n",
    "\n",
    "spark.sql('select * from lesson43_update').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c056d15e-27c0-45b5-98b2-f8a8b9a5273d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:55:01.509681Z",
     "iopub.status.busy": "2026-02-11T04:55:01.508344Z",
     "iopub.status.idle": "2026-02-11T04:55:02.668483Z",
     "shell.execute_reply": "2026-02-11T04:55:02.667473Z",
     "shell.execute_reply.started": "2026-02-11T04:55:01.509625Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>4</td><td>2026-02-11 04:52:14</td><td>null</td><td>null</td><td>MERGE</td><td>{predicate -&gt; (sp...</td><td>null</td><td>null</td><td>null</td><td>3</td><td>Serializable</td><td>false</td><td>{numTargetRowsCop...</td><td>null</td><td>Apache-Spark/3.3....</td></tr>\n",
       "<tr><td>3</td><td>2026-02-10 13:08:44</td><td>null</td><td>null</td><td>UPDATE</td><td>{predicate -&gt; (id...</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>false</td><td>{numRemovedFiles ...</td><td>null</td><td>Apache-Spark/3.3....</td></tr>\n",
       "<tr><td>2</td><td>2026-02-10 13:06:37</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>null</td><td>null</td><td>null</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 2, n...</td><td>null</td><td>Apache-Spark/3.3....</td></tr>\n",
       "<tr><td>1</td><td>2026-02-10 13:06:17</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, ...</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, n...</td><td>null</td><td>Apache-Spark/3.3....</td></tr>\n",
       "<tr><td>0</td><td>2026-02-10 12:56:41</td><td>null</td><td>null</td><td>CREATE OR REPLACE...</td><td>{isManaged -&gt; tru...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>true</td><td>{}</td><td>null</td><td>Apache-Spark/3.3....</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
       "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
       "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
       "|      4|2026-02-11 04:52:14|  null|    null|               MERGE|{predicate -> (sp...|null|    null|     null|          3|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
       "|      3|2026-02-10 13:08:44|  null|    null|              UPDATE|{predicate -> (id...|null|    null|     null|          2|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
       "|      2|2026-02-10 13:06:37|  null|    null|               WRITE|{mode -> Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.3....|\n",
       "|      1|2026-02-10 13:06:17|  null|    null|               WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 1, n...|        null|Apache-Spark/3.3....|\n",
       "|      0|2026-02-10 12:56:41|  null|    null|CREATE OR REPLACE...|{isManaged -> tru...|null|    null|     null|       null|  Serializable|         true|                  {}|        null|Apache-Spark/3.3....|\n",
       "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('describe history module_4.lesson43')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51765f8-e987-4d3e-8b56-87ad15c430da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T04:56:28.746765Z",
     "iopub.status.busy": "2026-02-11T04:56:28.745417Z",
     "iopub.status.idle": "2026-02-11T04:56:35.590099Z",
     "shell.execute_reply": "2026-02-11T04:56:35.589154Z",
     "shell.execute_reply.started": "2026-02-11T04:56:28.746708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>run_params</th><th>status</th><th>status_message</th></tr>\n",
       "<tr><td>1</td><td>lala1</td><td>lala2</td><td>lala3</td></tr>\n",
       "<tr><td>3</td><td>lala1</td><td>lala2</td><td>lala3</td></tr>\n",
       "<tr><td>2</td><td>lala1</td><td>lala2</td><td>lala3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+----------+------+--------------+\n",
       "| id|run_params|status|status_message|\n",
       "+---+----------+------+--------------+\n",
       "|  1|     lala1| lala2|         lala3|\n",
       "|  3|     lala1| lala2|         lala3|\n",
       "|  2|     lala1| lala2|         lala3|\n",
       "+---+----------+------+--------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_16 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_3 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_8 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_26 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_40 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_40 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_44 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_24 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_31 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_1 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_16 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_18 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_21 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_35 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_41 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_20 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_32 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_6 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_42 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_14 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_37 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_37 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_20 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_17 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_49 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_39 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_23 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_28 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_25 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_9 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_38 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_38 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_9 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_31 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_4 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_5 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_30 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_5 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_37 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_29 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_48 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_9 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_28 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_13 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_35 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_44 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_1 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_1 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_7 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_39 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_49 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_7 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_20 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_26 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_2 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_10 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_19 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_27 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_34 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_48 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_41 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_38 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_12 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_0 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_36 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_6 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_2 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_12 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_33 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_27 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_46 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_40 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_17 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_29 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_25 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_45 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_16 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_25 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_12 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_47 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_32 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_18 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_21 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_29 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_40 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_5 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_48 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_30 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_32 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_13 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_45 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_43 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_39 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_37 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_3 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_4 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_36 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_48 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_42 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_49 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_22 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_25 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_24 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_14 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_29 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_2 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_10 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_15 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_6 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_43 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_43 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_45 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_42 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_3 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_27 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_13 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_46 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_34 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_10 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_9 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_45 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_22 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_34 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_2 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_7 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_22 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_11 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_39 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_35 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_27 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_17 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_32 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_47 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_47 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_33 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_19 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_31 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_33 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_33 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_46 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_44 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_30 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_43 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_15 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_49 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_4 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_8 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_47 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_41 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_3 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_4 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_36 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_46 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_28 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_21 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_20 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_11 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_8 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_8 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_18 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_6 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_12 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_14 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_5 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_22 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_14 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_0 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_16 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_23 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_23 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_21 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_26 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_42 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_18 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_23 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_26 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_15 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_28 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_19 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_1 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_24 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_44 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_30 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_19 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_34 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_17 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_15 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_24 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_13 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_11 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_31 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_154_38 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_7 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_11 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_10 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_36 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_35 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_0 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_116_0 !\n",
      "26/02/11 05:57:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_150_41 !\n",
      "26/02/11 05:57:13 WARN TransportChannelHandler: Exception in connection from /10.40.0.7:32960\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "26/02/11 05:57:13 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.40.0.7:32960 is closed\n",
      "26/02/11 05:57:13 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 1 at RPC address 10.40.0.7:48844, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "26/02/11 05:57:13 ERROR YarnScheduler: Lost executor 1 on rc1a-dataproc-c-87klsv39dtvn1t4o.mdb.yandexcloud.net: Executor Process Lost\n",
      "26/02/11 05:57:14 ERROR YarnClientSchedulerBackend: YARN application has exited unexpectedly with state KILLED! Check the YARN application logs for more details.\n",
      "26/02/11 05:57:14 ERROR YarnClientSchedulerBackend: Diagnostics message: Application application_1770784561522_0002 was killed by user ubuntu at 10.40.0.9\n",
      "26/02/11 05:57:14 ERROR TransportClient: Failed to send RPC RPC 8733873348477278457 to /10.40.0.7:32960: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "26/02/11 05:57:14 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(),Map(),Map(),Set()) to AM was unsuccessful\n",
      "java.io.IOException: Failed to send RPC RPC 8733873348477278457 to /10.40.0.7:32960: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "26/02/11 05:57:14 ERROR Utils: Uncaught exception in thread YARN application state monitor\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:789)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:114)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:178)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:931)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2105)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2105)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:125)\n",
      "Caused by: java.io.IOException: Failed to send RPC RPC 8733873348477278457 to /10.40.0.7:32960: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from module_4.lesson43 version as of 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85da8e2-4662-4209-a0cf-7a692972d622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
